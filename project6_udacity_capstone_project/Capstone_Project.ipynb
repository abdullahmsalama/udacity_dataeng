{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Purpose of the project\n",
    "The purpose of the projecs is to study the following:<br />\n",
    "1- The relation between temperature of departure country at the time and the volume of travellers.<br />\n",
    "2- The relation between the temperature of departure country and temperature of arrival state at the time of travel.<br />\n",
    "3- The seasonality of travel and the visa type related to (ex. which month of the year has most given tourist visa type).\n",
    "\n",
    "\n",
    "### Step 2: Project Steps\n",
    "1- Use pandas to analyze a sample of the data from each csv sample files.<br />\n",
    "2- Use Spark to load the full data into dfs.<br />\n",
    "3- Analyze the immigration data, city demographics, airport codes and global temperature data to capture missing data and devise a strategy to clean.<br />\n",
    "4- Clean the data. <br />\n",
    "5- Create the tables (fact and dimension tables) <br />\n",
    "   5.1- \"country temperature\" dimension table: this table contain the country and its monthly temperature based on   \n",
    "   the country temperature reported in global temperature data, this table with the next table can help understand the flow of tourism/immigration based on the temperature of the departure country and arrival in us states. The link to the    fact table is through the code of the country of residence.<br />\n",
    "   5.2- \"us state temperature\" dimension table: this table will contain the state, state code and its monthly temperature based on the temperature reported in the us state in the global temperature data, this table with the previous table can help understand the flow of tourism/immigration based on the temperature of the departure country and arrival in us states. The link to the fact table is through the state code (i94addr - USA State of arrival).<br />\n",
    "   5.3- \"usa demographics\" dimension table which is constructed from the demographics data, the link to the fact table is         through the state code.<br />\n",
    "   5.4- \"travel month\" dimension table which is constructed directly from I94 immigration data. It includes the departure country, arrival city and month of travel. It links to the fact         table through the travel information unique identifier \"cicid\", <br />\n",
    "   5.5- \"Visa type\" dimension table which maps the shortened \n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading the immigration data sample\n",
    "df_immigration_sample = pd.read_csv('immigration_data_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
       "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
       "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_immigration_sample.head())\n",
    "df_immigration_sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Now, to understand what different columns represent\n",
    "\n",
    "<b><i>Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">cicid</td><td class=\"tg-0pky\">Unique record ID</td>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">year - 4 digits (xyzw)</td>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">month (1-12)</td>\n",
    " <tr><td class=\"tg-0pky\">i94cit</td><td class=\"tg-0pky\">3 digit code for immigrant country of birth</td>\n",
    " <tr><td class=\"tg-0pky\">i94res</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td>\n",
    " <tr><td class=\"tg-0pky\">insnum</td><td class=\"tg-0pky\">INS number</td>\n",
    " <tr><td class=\"tg-0pky\">airline</td><td class=\"tg-0pky\">Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">admnum</td><td class=\"tg-0pky\">Admission Number</td>\n",
    " <tr><td class=\"tg-0pky\">fltno</td><td class=\"tg-0pky\">Flight number of Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">visatype</td><td class=\"tg-0pky\">Class of admission legally admitting the non-immigrant to temporarily stay in U.S.</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WT     443\n",
       "B2     356\n",
       "WB      91\n",
       "B1      61\n",
       "GMT     27\n",
       "F1      10\n",
       "CP       5\n",
       "F2       3\n",
       "E2       3\n",
       "M1       1\n",
       "Name: visatype, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_sample.visatype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    831\n",
       "1.0    155\n",
       "3.0     14\n",
       "Name: i94visa, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_sample.i94visa.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FL    188\n",
       "CA    163\n",
       "NY    161\n",
       "HI     53\n",
       "TX     42\n",
       "NV     34\n",
       "IL     31\n",
       "GU     27\n",
       "MA     26\n",
       "NJ     20\n",
       "WA     19\n",
       "GA     19\n",
       "VA     13\n",
       "DC     12\n",
       "NE     12\n",
       "MD     11\n",
       "PA     10\n",
       "MI      9\n",
       "NC      9\n",
       "LA      8\n",
       "TN      7\n",
       "IN      7\n",
       "CT      6\n",
       "AL      5\n",
       "CO      5\n",
       "OH      5\n",
       "AZ      5\n",
       "SC      3\n",
       "MN      3\n",
       "MP      3\n",
       "OR      2\n",
       "VT      2\n",
       "UN      2\n",
       "MO      2\n",
       "IA      1\n",
       "VQ      1\n",
       "ID      1\n",
       "AR      1\n",
       "OK      1\n",
       "NH      1\n",
       "RI      1\n",
       "ME      1\n",
       "PR      1\n",
       "SW      1\n",
       "WI      1\n",
       "TE      1\n",
       "KY      1\n",
       "KS      1\n",
       "NM      1\n",
       "MS      1\n",
       "UT      1\n",
       "Name: i94addr, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_sample.i94addr.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## We will use from here cicid, i94mon, i94res, i94addr, i94visa, visatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_immigration_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2847924"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_immigration_spark.write.parquet(\"sas_data\")\n",
    "df_immigration_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Now look at city demographic csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring;Maryland;33.8;40601;41862;82463;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy;Massachusetts;41.0;44129;49500;93629;41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover;Alabama;38.5;38040;46799;84839;4819;822...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga;California;34.5;88127;87105;1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark;New Jersey;34.6;138040;143873;281913;58...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count\n",
       "0  Silver Spring;Maryland;33.8;40601;41862;82463;...                                                                                                   \n",
       "1  Quincy;Massachusetts;41.0;44129;49500;93629;41...                                                                                                   \n",
       "2  Hoover;Alabama;38.5;38040;46799;84839;4819;822...                                                                                                   \n",
       "3  Rancho Cucamonga;California;34.5;88127;87105;1...                                                                                                   \n",
       "4  Newark;New Jersey;34.6;138040;143873;281913;58...                                                                                                   "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo = pd.read_csv('us-cities-demographics.csv')\n",
    "df_city_demo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Not quite right, seperate by ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "df_city_demo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## We will use the demographics data to map the state alpha code to the state name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Now look at country temperature csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439242</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>15.544</td>\n",
       "      <td>0.281</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439243</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>20.892</td>\n",
       "      <td>0.273</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439244</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>24.722</td>\n",
       "      <td>0.279</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439245</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>21.001</td>\n",
       "      <td>0.323</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439246</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>17.408</td>\n",
       "      <td>1.048</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>United States</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>74.56W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>687289 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "47555    1820-01-01               2.101                          3.217   \n",
       "47556    1820-02-01               6.926                          2.853   \n",
       "47557    1820-03-01              10.767                          2.395   \n",
       "47558    1820-04-01              17.989                          2.202   \n",
       "47559    1820-05-01              21.809                          2.036   \n",
       "...             ...                 ...                            ...   \n",
       "8439242  2013-05-01              15.544                          0.281   \n",
       "8439243  2013-06-01              20.892                          0.273   \n",
       "8439244  2013-07-01              24.722                          0.279   \n",
       "8439245  2013-08-01              21.001                          0.323   \n",
       "8439246  2013-09-01              17.408                          1.048   \n",
       "\n",
       "            City        Country Latitude Longitude  \n",
       "47555    Abilene  United States   32.95N   100.53W  \n",
       "47556    Abilene  United States   32.95N   100.53W  \n",
       "47557    Abilene  United States   32.95N   100.53W  \n",
       "47558    Abilene  United States   32.95N   100.53W  \n",
       "47559    Abilene  United States   32.95N   100.53W  \n",
       "...          ...            ...      ...       ...  \n",
       "8439242  Yonkers  United States   40.99N    74.56W  \n",
       "8439243  Yonkers  United States   40.99N    74.56W  \n",
       "8439244  Yonkers  United States   40.99N    74.56W  \n",
       "8439245  Yonkers  United States   40.99N    74.56W  \n",
       "8439246  Yonkers  United States   40.99N    74.56W  \n",
       "\n",
       "[687289 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "df_temperature[df_temperature['Country'] == 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### We'll use the data to calculate average temperature per country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## First I would like to begin for the temperature data because I noticed it included much more data than I would need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1743-11-01'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2013-09-01'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows are 8599212\n"
     ]
    }
   ],
   "source": [
    "display(df_temperature['dt'].min())\n",
    "display(df_temperature['dt'].max())\n",
    "\n",
    "print(\"The number of rows are\", len(df_temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### As we see that is just too much, we wouldn't need temperature data for 270 years to calculate the average for our task. 50 years is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1963-09-01'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2013-09-01'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the number of rows are 2109510\n"
     ]
    }
   ],
   "source": [
    "df_temperature = df_temperature[df_temperature['dt'] >= '1963-09-01']\n",
    "\n",
    "display(df_temperature['dt'].min())\n",
    "display(df_temperature['dt'].max())\n",
    "\n",
    "print(\"Now the number of rows are\", len(df_temperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now we check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                  0\n",
       "AverageTemperature               3070\n",
       "AverageTemperatureUncertainty    3070\n",
       "City                                0\n",
       "Country                             0\n",
       "Latitude                            0\n",
       "Longitude                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6477</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Çorlu</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>27.69E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Çorum</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>34.08E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11924</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Öskemen</td>\n",
       "      <td>Kazakhstan</td>\n",
       "      <td>50.63N</td>\n",
       "      <td>82.39E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14242</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ürümqi</td>\n",
       "      <td>China</td>\n",
       "      <td>44.20N</td>\n",
       "      <td>87.20E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587519</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zouxian</td>\n",
       "      <td>China</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>117.35E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589604</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zunyi</td>\n",
       "      <td>China</td>\n",
       "      <td>28.13N</td>\n",
       "      <td>106.36E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592843</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zurich</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>47.42N</td>\n",
       "      <td>8.29E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8595972</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zuwarah</td>\n",
       "      <td>Libya</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>12.45E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3070 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "3238     2013-09-01                 NaN                            NaN   \n",
       "6477     2013-09-01                 NaN                            NaN   \n",
       "9606     2013-09-01                 NaN                            NaN   \n",
       "11924    2013-09-01                 NaN                            NaN   \n",
       "14242    2013-09-01                 NaN                            NaN   \n",
       "...             ...                 ...                            ...   \n",
       "8587519  2013-09-01                 NaN                            NaN   \n",
       "8589604  2013-09-01                 NaN                            NaN   \n",
       "8592843  2013-09-01                 NaN                            NaN   \n",
       "8595972  2013-09-01                 NaN                            NaN   \n",
       "8599211  2013-09-01                 NaN                            NaN   \n",
       "\n",
       "            City      Country Latitude Longitude  \n",
       "3238       Århus      Denmark   57.05N    10.33E  \n",
       "6477       Çorlu       Turkey   40.99N    27.69E  \n",
       "9606       Çorum       Turkey   40.99N    34.08E  \n",
       "11924    Öskemen   Kazakhstan   50.63N    82.39E  \n",
       "14242     Ürümqi        China   44.20N    87.20E  \n",
       "...          ...          ...      ...       ...  \n",
       "8587519  Zouxian        China   36.17N   117.35E  \n",
       "8589604    Zunyi        China   28.13N   106.36E  \n",
       "8592843   Zurich  Switzerland   47.42N     8.29E  \n",
       "8595972  Zuwarah        Libya   32.95N    12.45E  \n",
       "8599211   Zwolle  Netherlands   52.24N     5.26E  \n",
       "\n",
       "[3070 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature[df_temperature.AverageTemperature.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### It seems that there is a common error in 2013-09-01, but let's see more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013-09-01    3070\n",
       "Name: dt, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_temperature[df_temperature.AverageTemperature.isnull()]['dt'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So all from the same day, and it is the last date in the dataset. We'll just act as this value never existed as we have alot of data. We could have tried to fill it using average values from previous years but that will not change anything when we calculate the average temperature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the number of rows are 2106440\n"
     ]
    }
   ],
   "source": [
    "df_temperature = df_temperature[df_temperature.AverageTemperature.notnull()]\n",
    "print(\"Now the number of rows are\", len(df_temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature['dt']= pd.to_datetime(df_temperature['dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now the demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### For our task, we only need the city and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>MD</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City State_Code          State\n",
       "0     Silver Spring         MD       Maryland\n",
       "1            Quincy         MA  Massachusetts\n",
       "2            Hoover         AL        Alabama\n",
       "3  Rancho Cucamonga         CA     California\n",
       "4            Newark         NJ     New Jersey"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo['State_Code'] = df_city_demo['State Code']\n",
    "df_city_demo = df_city_demo.drop(['State Code'], axis = 1)\n",
    "df_city_demo = df_city_demo[['City', 'State_Code', 'State']]\n",
    "df_city_demo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check for null enteries for this dataset also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City          0\n",
       "State_Code    0\n",
       "State         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### That's convenient, now drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>MD</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>NJ</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City State_Code          State\n",
       "0     Silver Spring         MD       Maryland\n",
       "1            Quincy         MA  Massachusetts\n",
       "2            Hoover         AL        Alabama\n",
       "3  Rancho Cucamonga         CA     California\n",
       "4            Newark         NJ     New Jersey"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo = df_city_demo.drop_duplicates()\n",
    "df_city_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Wilmington</td>\n",
       "      <td>DE</td>\n",
       "      <td>Delaware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Glendale</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Westminster</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>Albany</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>SC</td>\n",
       "      <td>South Carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>Aurora</td>\n",
       "      <td>CO</td>\n",
       "      <td>Colorado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>Bloomington</td>\n",
       "      <td>MN</td>\n",
       "      <td>Minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>Lafayette</td>\n",
       "      <td>IN</td>\n",
       "      <td>Indiana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             City State_Code           State\n",
       "177    Wilmington         DE        Delaware\n",
       "210      Lakewood         CA      California\n",
       "238      Glendale         CA      California\n",
       "242        Peoria         AZ         Arizona\n",
       "250   Westminster         CA      California\n",
       "...           ...        ...             ...\n",
       "1470       Albany         NY        New York\n",
       "1543     Columbia         SC  South Carolina\n",
       "1621       Aurora         CO        Colorado\n",
       "1629  Bloomington         MN       Minnesota\n",
       "2573    Lafayette         IN         Indiana\n",
       "\n",
       "[29 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo[df_city_demo[['City']].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State_Code</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>MD</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>MO</td>\n",
       "      <td>Missouri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>SC</td>\n",
       "      <td>South Carolina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          City State_Code           State\n",
       "62    Columbia         MD        Maryland\n",
       "874   Columbia         MO        Missouri\n",
       "1543  Columbia         SC  South Carolina"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo[df_city_demo.City == 'Columbia']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So these are actually different cities with the same name in the US, quite confusing. Let's check how is it reported in the temperature df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So it is very difficult to know which Columbia the dataset is talking about (unless we check the latitude and longitude), which is very tedious. Alternatively we'll load another file for the us states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7458</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>10.722</td>\n",
       "      <td>2.898</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7459</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7460</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7461</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7462</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  AverageTemperature  AverageTemperatureUncertainty    State  \\\n",
       "7458  1743-11-01              10.722                          2.898  Alabama   \n",
       "7459  1743-12-01                 NaN                            NaN  Alabama   \n",
       "7460  1744-01-01                 NaN                            NaN  Alabama   \n",
       "7461  1744-02-01                 NaN                            NaN  Alabama   \n",
       "7462  1744-03-01                 NaN                            NaN  Alabama   \n",
       "\n",
       "            Country  \n",
       "7458  United States  \n",
       "7459  United States  \n",
       "7460  United States  \n",
       "7461  United States  \n",
       "7462  United States  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature_cities = df_temperature\n",
    "\n",
    "df_temperature_usstates = pd.read_csv('GlobalLandTemperaturesByState.csv')\n",
    "df_temperature_usstates = df_temperature_usstates[df_temperature_usstates['Country'] == \"United States\"]\n",
    "df_temperature_usstates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Since it is the same dataset, do the same operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10096</th>\n",
       "      <td>1963-09-01</td>\n",
       "      <td>23.083</td>\n",
       "      <td>0.283</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>1963-10-01</td>\n",
       "      <td>18.871</td>\n",
       "      <td>0.239</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>1963-11-01</td>\n",
       "      <td>12.082</td>\n",
       "      <td>0.259</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>1963-12-01</td>\n",
       "      <td>3.331</td>\n",
       "      <td>0.102</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>1964-01-01</td>\n",
       "      <td>6.090</td>\n",
       "      <td>0.291</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  AverageTemperature  AverageTemperatureUncertainty    State  \\\n",
       "10096 1963-09-01              23.083                          0.283  Alabama   \n",
       "10097 1963-10-01              18.871                          0.239  Alabama   \n",
       "10098 1963-11-01              12.082                          0.259  Alabama   \n",
       "10099 1963-12-01               3.331                          0.102  Alabama   \n",
       "10100 1964-01-01               6.090                          0.291  Alabama   \n",
       "\n",
       "             Country  \n",
       "10096  United States  \n",
       "10097  United States  \n",
       "10098  United States  \n",
       "10099  United States  \n",
       "10100  United States  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature_usstates = df_temperature_usstates[df_temperature_usstates['dt'] >= '1963-09-01']\n",
    "df_temperature_usstates = df_temperature_usstates[df_temperature_usstates.AverageTemperature.notnull()]\n",
    "df_temperature_usstates['dt']= pd.to_datetime(df_temperature_usstates['dt'])\n",
    "df_temperature_usstates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now, with the immigration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Since we are interested in the following columns only, then we'll remove other columns to make all next processes faster. The columns are: cicid, i94yr, i94mon, i94city, i94res, arrdate, i94mode, i94addr, i94visa, visatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration_spark = spark.sql(\"\"\"select cicid, i94cit,\n",
    "i94res, arrdate, i94mode, i94addr, i94visa, visatype\n",
    "from immigration_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|      admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "|  7.0|2016.0|   1.0| 101.0| 101.0|    BOS|20465.0|    1.0|     MA|   null|  20.0|    3.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1996.0|     D/S|     M|  null|     LH|3.46608285E8|  424|      F1|\n",
      "|  8.0|2016.0|   1.0| 101.0| 101.0|    BOS|20465.0|    1.0|     MA|   null|  20.0|    3.0|  1.0|    null|    null| null|      T|   null|   null|   null| 1996.0|     D/S|     M|  null|     LH|3.46627585E8|  424|      F1|\n",
      "|  9.0|2016.0|   1.0| 101.0| 101.0|    BOS|20469.0|    1.0|     CT|20480.0|  17.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 1999.0|07152016|     F|  null|     AF|3.81092385E8|  338|      B2|\n",
      "| 10.0|2016.0|   1.0| 101.0| 101.0|    BOS|20469.0|    1.0|     CT|20499.0|  45.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 1971.0|07152016|     F|  null|     AF|3.81087885E8|  338|      B2|\n",
      "| 11.0|2016.0|   1.0| 101.0| 101.0|    BOS|20469.0|    1.0|     CT|20499.0|  12.0|    2.0|  1.0|    null|    null| null|      T|      N|   null|      M| 2004.0|07152016|     M|  null|     AF|3.81078685E8|  338|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from immigration_table limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2847924"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Good, now only filter with the air travel and remove the column since I am also interested in this type of travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration_spark = spark.sql(\"\"\"select cicid, i94cit,\n",
    "i94res, arrdate, i94addr, i94visa, visatype\n",
    "from immigration_table where i94mode = 1\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-------+-------+-------+--------+\n",
      "|cicid|i94cit|i94res|arrdate|i94addr|i94visa|visatype|\n",
      "+-----+------+------+-------+-------+-------+--------+\n",
      "|  7.0| 101.0| 101.0|20465.0|     MA|    3.0|      F1|\n",
      "|  8.0| 101.0| 101.0|20465.0|     MA|    3.0|      F1|\n",
      "|  9.0| 101.0| 101.0|20469.0|     CT|    2.0|      B2|\n",
      "| 10.0| 101.0| 101.0|20469.0|     CT|    2.0|      B2|\n",
      "| 11.0| 101.0| 101.0|20469.0|     CT|    2.0|      B2|\n",
      "+-----+------+------+-------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")\n",
    "spark.sql(\"\"\"\n",
    "select * from immigration_table limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2761448"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now get the arrival month and year from the arrdate, and since arrdate represents the number of days since 1960. This information could be the same as i94yr and i94mon, but since I am not sure. I will calculate from arrdate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_spark = spark.sql(\"\"\"select *, date_add(to_date('1960-01-01'), arrdate) as arrival_date\n",
    "from immigration_table\"\"\")\n",
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now, rename and remove unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_spark = spark.sql(\"\"\"select cicid, arrival_date, i94cit as immig_country_birth_code,\n",
    "i94res as immig_country_residence_code, i94addr as arrival_usa_state, i94visa as categories_arrival_visa,\n",
    "visatype as visa_category\n",
    "from immigration_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT cicid)|\n",
      "+---------------------+\n",
      "|              2761448|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")\n",
    "# First we check the uniqueness of the cicid\n",
    "spark.sql(\"\"\"\n",
    "select count (distinct cicid)\n",
    "from immigration_table\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2761448"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So as expected cicid is unique and is perfect to be the primary key of the fact table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|state_address_length|occurence|\n",
      "+--------------------+---------+\n",
      "|                   2|  2620906|\n",
      "|                null|   140421|\n",
      "|                   1|      121|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select length (arrival_usa_state) as state_address_length, count(*) as occurence\n",
    "from immigration_table\n",
    "group by state_address_length\n",
    "order by occurence desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|arrival_usa_state|\n",
      "+-----------------+\n",
      "|                C|\n",
      "|                X|\n",
      "|                S|\n",
      "|                N|\n",
      "|                N|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select arrival_usa_state from immigration_table where length (arrival_usa_state) = 1 limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    596\n",
       "Name: State_Code, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_demo['State_Code'].astype(str).map(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So as expected, the state codes has to be 2 characters, and the null values and 1 character values in the immigration table are not right.\n",
    "\n",
    "### Just one more thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|count(DISTINCT arrival_usa_state)|\n",
      "+---------------------------------+\n",
      "|                              410|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(distinct(arrival_usa_state)) from immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So great, all the arrival states in the immigration table are included in the city demographics data frame\n",
    "### So let's only include state codes with 2 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+-----------------------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|categories_arrival_visa|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+-----------------------+-------------+\n",
      "|  7.0|  2016-01-12|                   101.0|                       101.0|               MA|                    3.0|           F1|\n",
      "|  8.0|  2016-01-12|                   101.0|                       101.0|               MA|                    3.0|           F1|\n",
      "|  9.0|  2016-01-16|                   101.0|                       101.0|               CT|                    2.0|           B2|\n",
      "| 10.0|  2016-01-16|                   101.0|                       101.0|               CT|                    2.0|           B2|\n",
      "| 11.0|  2016-01-16|                   101.0|                       101.0|               CT|                    2.0|           B2|\n",
      "| 12.0|  2016-01-21|                   101.0|                       101.0|               MA|                    2.0|           B2|\n",
      "| 15.0|  2016-01-24|                   101.0|                       101.0|               MA|                    3.0|           F1|\n",
      "| 17.0|  2016-01-27|                   101.0|                       101.0|               MA|                    2.0|           B2|\n",
      "| 18.0|  2016-01-27|                   101.0|                       101.0|               MA|                    2.0|           B2|\n",
      "| 20.0|  2016-01-20|                   101.0|                       101.0|               IL|                    2.0|           B2|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+-----------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_spark = spark.sql(\"\"\"\n",
    "select * from immigration_table where length (arrival_usa_state) = 2\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "select * from immigration_table limit 10\"\"\").show()\n",
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Since we also know what the 3 visa number categories refer to, so we can convert it for easier read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|categories_arrival_visa|\n",
      "+-----------------------+\n",
      "|                    1.0|\n",
      "|                    3.0|\n",
      "|                    2.0|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select distinct(categories_arrival_visa) from immigration_table\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_spark = spark.sql(\"\"\" select cicid, arrival_date,\n",
    "                        immig_country_birth_code, immig_country_residence_code,\n",
    "                        arrival_usa_state, case \n",
    "                        when categories_arrival_visa = 1.0 then 'Business' \n",
    "                        when categories_arrival_visa = 2.0 then 'Leisure'\n",
    "                        when categories_arrival_visa = 3.0 then 'Study'\n",
    "                        else 'Unknown' end\n",
    "                        as visa_type, visa_category \n",
    "                        from immigration_table\"\"\")\n",
    "\n",
    "df_immigration_spark.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now let's make sure that each visa_category belongs to a unique visa_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------------------+\n",
      "|visa_type|visa_category|number_of_occurences|\n",
      "+---------+-------------+--------------------+\n",
      "| Business|           B1|              186997|\n",
      "| Business|           E1|                5237|\n",
      "| Business|           E2|               26994|\n",
      "| Business|          GMB|                 556|\n",
      "| Business|            I|                3350|\n",
      "| Business|           I1|                 196|\n",
      "| Business|           WB|              276406|\n",
      "|  Leisure|           B2|              871889|\n",
      "|  Leisure|           CP|                 994|\n",
      "|  Leisure|          CPL|                  28|\n",
      "|  Leisure|          GMT|               95267|\n",
      "|  Leisure|          SBP|                   5|\n",
      "|  Leisure|           WT|              779660|\n",
      "|    Study|           F1|              362039|\n",
      "|    Study|           F2|                9270|\n",
      "|    Study|           M1|                1968|\n",
      "|    Study|           M2|                  50|\n",
      "+---------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select visa_type, visa_category, count(*) as number_of_occurences\n",
    "from immigration_table\n",
    "group by visa_type, visa_category\n",
    "order by visa_type, visa_category\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### So they are unique, B1 for example is a cateogory for business type visa, while B2 is a category for a Leisure/Tourist type visa and both are temporary, whill research more about the other types when we get a specific insight and we try and analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|  7.0|  2016-01-12|                   101.0|                       101.0|               MA|    Study|           F1|\n",
      "|  8.0|  2016-01-12|                   101.0|                       101.0|               MA|    Study|           F1|\n",
      "|  9.0|  2016-01-16|                   101.0|                       101.0|               CT|  Leisure|           B2|\n",
      "| 10.0|  2016-01-16|                   101.0|                       101.0|               CT|  Leisure|           B2|\n",
      "| 11.0|  2016-01-16|                   101.0|                       101.0|               CT|  Leisure|           B2|\n",
      "| 12.0|  2016-01-21|                   101.0|                       101.0|               MA|  Leisure|           B2|\n",
      "| 15.0|  2016-01-24|                   101.0|                       101.0|               MA|    Study|           F1|\n",
      "| 17.0|  2016-01-27|                   101.0|                       101.0|               MA|  Leisure|           B2|\n",
      "| 18.0|  2016-01-27|                   101.0|                       101.0|               MA|  Leisure|           B2|\n",
      "| 20.0|  2016-01-20|                   101.0|                       101.0|               IL|  Leisure|           B2|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check for null values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table where arrival_date is NULL limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table where immig_country_birth_code is NULL limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table where immig_country_residence_code is NULL limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table where arrival_usa_state is NULL limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table where visa_type is NULL limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "|cicid|arrival_date|immig_country_birth_code|immig_country_residence_code|arrival_usa_state|visa_type|visa_category|\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "+-----+------------+------------------------+----------------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table where visa_category is NULL limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Good, no Null values in any of the columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Finally, since the immigrant country birth country and residence country are in codes. We have to have the mapping for it to be informative. Thus I searched and found countries.csv which maps the country codes to their respective names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code      country\n",
       "0   582      MEXICO \n",
       "1   236  AFGHANISTAN\n",
       "2   101      ALBANIA\n",
       "3   316      ALGERIA\n",
       "4   102      ANDORRA"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_codes = pd.read_csv('countries.csv')\n",
    "df_country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code       False\n",
       "country    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_codes.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "As explained in the beginning in step 2, we will use the immigration table to create the fact table \"immigration_fact\" with the below columns:\n",
    "- cicid (primary key)\n",
    "- arrival_date\n",
    "- immig_country_birth\n",
    "- immig_country_residence\n",
    "- arrival_usa_state_alpha_code\n",
    "- visa_type\n",
    "- visa_category\n",
    "\n",
    "Now comes the dimension tables, starting with \"country_monthly_temperature_dim\", which will include monthly average temperature of each country (to be used in relation to departure countries temperature). It links to the fact table through the country in the dimension table and through residence and birth countries in the fact table. It will include these columns:\n",
    "\n",
    "- country\n",
    "- month\n",
    "- average_temperature\n",
    "- - primary keys (country, month)\n",
    "\n",
    "Also, \"us_state_monthly_temperature_dim\" which has the same purpose as the previous but was detached due to non-unqiue naming of cities between different us states. Links to the fact table via state. The columns are:\n",
    "\n",
    "- state_alpha_code\n",
    "- month\n",
    "- average_temperature\n",
    "- - primary keys (state, month)\n",
    "\n",
    "\"usa_states_dim\" dervied from the demographics is also important and links to the fact table via state code, and the columnns are:\n",
    "\n",
    "- state_alpha_code (primary key)\n",
    "- state\n",
    "\n",
    "Finally, \"arrival_dates_dim\", which links to the fact table via arrival_date, and the columns are:\n",
    "\n",
    "- date (primary key)\n",
    "- year\n",
    "- month\n",
    "- week\n",
    "- day\n",
    "- day_of_week\n",
    "- season\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Here is how the ETL is designed:\n",
    "\n",
    "1- E (Extraction): Read the data from the SAS data and csv files.\n",
    "\n",
    "2- TL (Transformation and loading): Do the following to construct each table:\n",
    "\n",
    "\"immigration_fact\"\n",
    "\n",
    "Using the immigration data and countries data (from countries.csv):\n",
    "\n",
    "- Drop \"i94port\" because we are not interested in airports in the task we have chosen. (Done)\n",
    "- Drop \"i94yr\" and \"i94mon\" as we only need the data which is \"arrdate\". (Done)\n",
    "- Filter by \"i94mode\" = 1 (air travel) and then drop the column. (Done)\n",
    "- Drop \"depdate\" as we are not interested in the departure date from USA. (Done)\n",
    "- Drop \"count\", \"dtadfile\", \"visapost\", \"occup\", \"entdepa\", \"entdepd\", \"entdepu\", \"matflag\", \"biryear\", \"gender\", \"isnum\", \"airline\", \"amnum\", \"fltno\" as they are not needed for our task. (Done)\n",
    "- Replace the country codes with their corresponding country names. (Not Yet Done)\n",
    "- Load the data to the fact table. (Not Yet Done)\n",
    "- Write to parquet. (Not Yet Done)\n",
    "\n",
    "\"country_monthly_temperature_dim\"\n",
    "\n",
    "Using the city_temperature data:\n",
    "- Drop unneeded temperatures of more than 50 years in the past. (Done)\n",
    "- Handle the null temperatures. (Done)\n",
    "- Aggregate the city temperatures to calculate the monthly average temperature for each country. (Not Yet Done)\n",
    "- Load the data to the dimension table. (Not Yet Done)\n",
    "- Write to parquet. (Not Yet Done)\n",
    "\n",
    "\"us_state_monthly_temperature_dim\"\n",
    "Using the state_temperature data and the city_demographics data:\n",
    "- Drop unneeded temperatures of more than 50 years in the past. (Done)\n",
    "- Handle the null temperatures. (Done)\n",
    "- Map the states to state_codes using city_demographics to be inserted in the dimension table as alpha codes for each state. (Not Yet Done)\n",
    "- Load the data to the dimension table. (Not Yet Done)\n",
    "- Write to parquet. (Not Yet Done)\n",
    "\n",
    "\"usa_states_dim\"\n",
    "Using the demographics data (All Not Yet Done):\n",
    "- Filter and get the wanted columns (the state code and the state name).\n",
    "- Load the data to the dimension table.\n",
    "- Write to parquet.\n",
    "\n",
    "\"arrival_dates_dim\"\n",
    "Using the immigration data (All Not Yet Done):\n",
    "- Extract all the columns from the arrival date (year, month, week, day, day_of_week and season)\n",
    "- Load the data to the dimension table.\n",
    "- Write to parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### First, the immigration fact table\n",
    "To be done:\n",
    "\n",
    "- Replace the country codes with their corresponding country names. \n",
    "- Load the data to the fact table.\n",
    "- Write to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------------+-----------------------+-----------------+---------+-------------+\n",
      "|  cicid|arrival_date|immig_country_birth|immig_country_residence|arrival_usa_state|visa_type|visa_category|\n",
      "+-------+------------+-------------------+-----------------------+-----------------+---------+-------------+\n",
      "|88402.0|  2016-01-08|           MONGOLIA|               MONGOLIA|               GU|  Leisure|           B2|\n",
      "|88403.0|  2016-01-21|           MONGOLIA|               MONGOLIA|               MA|    Study|           F1|\n",
      "|88406.0|  2016-01-02|           MONGOLIA|               MONGOLIA|               MO|    Study|           F1|\n",
      "|88407.0|  2016-01-02|           MONGOLIA|               MONGOLIA|               UT|    Study|           F1|\n",
      "|88408.0|  2016-01-04|           MONGOLIA|               MONGOLIA|               IL|  Leisure|           B2|\n",
      "|88409.0|  2016-01-05|           MONGOLIA|               MONGOLIA|               IL|    Study|           F1|\n",
      "|88410.0|  2016-01-06|           MONGOLIA|               MONGOLIA|               AE|    Study|           F1|\n",
      "|88411.0|  2016-01-06|           MONGOLIA|               MONGOLIA|               AE|    Study|           F1|\n",
      "|88412.0|  2016-01-06|           MONGOLIA|               MONGOLIA|               WI|  Leisure|           B2|\n",
      "|88413.0|  2016-01-06|           MONGOLIA|               MONGOLIA|               WI|  Leisure|           B2|\n",
      "+-------+------------+-------------------+-----------------------+-----------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code      country\n",
       "0   582      MEXICO \n",
       "1   236  AFGHANISTAN"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_codes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 582|    MEXICO |\n",
      "| 236|AFGHANISTAN|\n",
      "+----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_codes_spark = spark.createDataFrame(df_country_codes) \n",
    "df_country_codes_spark.printSchema()\n",
    "df_country_codes_spark.show(2)\n",
    "df_country_codes_spark.createOrReplaceTempView(\"country_codes_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Now adding the country of birth from the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`imm_t.immig_country_birth_code`' given input columns: [imm_t.visa_type, imm_t.immig_country_birth, imm_t.arrival_usa_state, imm_t.cicid, imm_t.immig_country_residence, cc_t.code, cc_t.country, imm_t.visa_category, imm_t.arrival_date]; line 5 pos 3;\\n'Project [ArrayBuffer(imm_t).*, 'cc_t.country AS immig_country_birth#935]\\n+- 'Join Inner, ('imm_t.immig_country_birth_code = code#895L)\\n   :- SubqueryAlias `imm_t`\\n   :  +- SubqueryAlias `immigration_table`\\n   :     +- Project [cicid#0, arrival_date#271, immig_country_birth#803, immig_country_residence#878, arrival_usa_state#282, visa_type#388, visa_category#284]\\n   :        +- SubqueryAlias `immigration_table`\\n   :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, immig_country_birth#803, country#793 AS immig_country_residence#878]\\n   :              +- Join Inner, (immig_country_residence_code#281 = cast(code#792L as double))\\n   :                 :- SubqueryAlias `imm_t`\\n   :                 :  +- SubqueryAlias `immigration_table`\\n   :                 :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, country#793 AS immig_country_birth#803]\\n   :                 :        +- Join Inner, (immig_country_birth_code#280 = cast(code#792L as double))\\n   :                 :           :- SubqueryAlias `imm_t`\\n   :                 :           :  +- SubqueryAlias `immigration_table`\\n   :                 :           :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, CASE WHEN (categories_arrival_visa#283 = cast(1.0 as double)) THEN Business WHEN (categories_arrival_visa#283 = cast(2.0 as double)) THEN Leisure WHEN (categories_arrival_visa#283 = cast(3.0 as double)) THEN Study ELSE Unknown END AS visa_type#388, visa_category#284]\\n   :                 :           :        +- SubqueryAlias `immigration_table`\\n   :                 :           :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, categories_arrival_visa#283, visa_category#284]\\n   :                 :           :              +- Filter (length(arrival_usa_state#282) = 2)\\n   :                 :           :                 +- SubqueryAlias `immigration_table`\\n   :                 :           :                    +- Project [cicid#0, arrival_date#271, i94cit#3 AS immig_country_birth_code#280, i94res#4 AS immig_country_residence_code#281, i94addr#8 AS arrival_usa_state#282, i94visa#11 AS categories_arrival_visa#283, visatype#27 AS visa_category#284]\\n   :                 :           :                       +- SubqueryAlias `immigration_table`\\n   :                 :           :                          +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27, date_add(to_date(1960-01-01, None), cast(arrdate#6 as int)) AS arrival_date#271]\\n   :                 :           :                             +- SubqueryAlias `immigration_table`\\n   :                 :           :                                +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                   +- Filter (i94mode#7 = cast(1 as double))\\n   :                 :           :                                      +- SubqueryAlias `immigration_table`\\n   :                 :           :                                         +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94mode#7, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                            +- SubqueryAlias `immigration_table`\\n   :                 :           :                                               +- Relation[cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] SasRelation(../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0)\\n   :                 :           +- SubqueryAlias `cc_t`\\n   :                 :              +- SubqueryAlias `country_codes_table`\\n   :                 :                 +- LogicalRDD [code#792L, country#793], false\\n   :                 +- SubqueryAlias `cc_t`\\n   :                    +- SubqueryAlias `country_codes_table`\\n   :                       +- LogicalRDD [code#792L, country#793], false\\n   +- SubqueryAlias `cc_t`\\n      +- SubqueryAlias `country_codes_table`\\n         +- LogicalRDD [code#895L, country#896], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`imm_t.immig_country_birth_code`' given input columns: [imm_t.visa_type, imm_t.immig_country_birth, imm_t.arrival_usa_state, imm_t.cicid, imm_t.immig_country_residence, cc_t.code, cc_t.country, imm_t.visa_category, imm_t.arrival_date]; line 5 pos 3;\n'Project [ArrayBuffer(imm_t).*, 'cc_t.country AS immig_country_birth#935]\n+- 'Join Inner, ('imm_t.immig_country_birth_code = code#895L)\n   :- SubqueryAlias `imm_t`\n   :  +- SubqueryAlias `immigration_table`\n   :     +- Project [cicid#0, arrival_date#271, immig_country_birth#803, immig_country_residence#878, arrival_usa_state#282, visa_type#388, visa_category#284]\n   :        +- SubqueryAlias `immigration_table`\n   :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, immig_country_birth#803, country#793 AS immig_country_residence#878]\n   :              +- Join Inner, (immig_country_residence_code#281 = cast(code#792L as double))\n   :                 :- SubqueryAlias `imm_t`\n   :                 :  +- SubqueryAlias `immigration_table`\n   :                 :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, country#793 AS immig_country_birth#803]\n   :                 :        +- Join Inner, (immig_country_birth_code#280 = cast(code#792L as double))\n   :                 :           :- SubqueryAlias `imm_t`\n   :                 :           :  +- SubqueryAlias `immigration_table`\n   :                 :           :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, CASE WHEN (categories_arrival_visa#283 = cast(1.0 as double)) THEN Business WHEN (categories_arrival_visa#283 = cast(2.0 as double)) THEN Leisure WHEN (categories_arrival_visa#283 = cast(3.0 as double)) THEN Study ELSE Unknown END AS visa_type#388, visa_category#284]\n   :                 :           :        +- SubqueryAlias `immigration_table`\n   :                 :           :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, categories_arrival_visa#283, visa_category#284]\n   :                 :           :              +- Filter (length(arrival_usa_state#282) = 2)\n   :                 :           :                 +- SubqueryAlias `immigration_table`\n   :                 :           :                    +- Project [cicid#0, arrival_date#271, i94cit#3 AS immig_country_birth_code#280, i94res#4 AS immig_country_residence_code#281, i94addr#8 AS arrival_usa_state#282, i94visa#11 AS categories_arrival_visa#283, visatype#27 AS visa_category#284]\n   :                 :           :                       +- SubqueryAlias `immigration_table`\n   :                 :           :                          +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27, date_add(to_date(1960-01-01, None), cast(arrdate#6 as int)) AS arrival_date#271]\n   :                 :           :                             +- SubqueryAlias `immigration_table`\n   :                 :           :                                +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27]\n   :                 :           :                                   +- Filter (i94mode#7 = cast(1 as double))\n   :                 :           :                                      +- SubqueryAlias `immigration_table`\n   :                 :           :                                         +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94mode#7, i94addr#8, i94visa#11, visatype#27]\n   :                 :           :                                            +- SubqueryAlias `immigration_table`\n   :                 :           :                                               +- Relation[cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] SasRelation(../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0)\n   :                 :           +- SubqueryAlias `cc_t`\n   :                 :              +- SubqueryAlias `country_codes_table`\n   :                 :                 +- LogicalRDD [code#792L, country#793], false\n   :                 +- SubqueryAlias `cc_t`\n   :                    +- SubqueryAlias `country_codes_table`\n   :                       +- LogicalRDD [code#792L, country#793], false\n   +- SubqueryAlias `cc_t`\n      +- SubqueryAlias `country_codes_table`\n         +- LogicalRDD [code#895L, country#896], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-682643fd1929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minner\u001b[0m \u001b[0mjoin\u001b[0m \u001b[0mcountry_codes_table\u001b[0m \u001b[0mcc_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mon\u001b[0m \u001b[0mimm_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimmig_country_birth_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \"\"\").createOrReplaceTempView(\"immigration_table\")\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`imm_t.immig_country_birth_code`' given input columns: [imm_t.visa_type, imm_t.immig_country_birth, imm_t.arrival_usa_state, imm_t.cicid, imm_t.immig_country_residence, cc_t.code, cc_t.country, imm_t.visa_category, imm_t.arrival_date]; line 5 pos 3;\\n'Project [ArrayBuffer(imm_t).*, 'cc_t.country AS immig_country_birth#935]\\n+- 'Join Inner, ('imm_t.immig_country_birth_code = code#895L)\\n   :- SubqueryAlias `imm_t`\\n   :  +- SubqueryAlias `immigration_table`\\n   :     +- Project [cicid#0, arrival_date#271, immig_country_birth#803, immig_country_residence#878, arrival_usa_state#282, visa_type#388, visa_category#284]\\n   :        +- SubqueryAlias `immigration_table`\\n   :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, immig_country_birth#803, country#793 AS immig_country_residence#878]\\n   :              +- Join Inner, (immig_country_residence_code#281 = cast(code#792L as double))\\n   :                 :- SubqueryAlias `imm_t`\\n   :                 :  +- SubqueryAlias `immigration_table`\\n   :                 :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, country#793 AS immig_country_birth#803]\\n   :                 :        +- Join Inner, (immig_country_birth_code#280 = cast(code#792L as double))\\n   :                 :           :- SubqueryAlias `imm_t`\\n   :                 :           :  +- SubqueryAlias `immigration_table`\\n   :                 :           :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, CASE WHEN (categories_arrival_visa#283 = cast(1.0 as double)) THEN Business WHEN (categories_arrival_visa#283 = cast(2.0 as double)) THEN Leisure WHEN (categories_arrival_visa#283 = cast(3.0 as double)) THEN Study ELSE Unknown END AS visa_type#388, visa_category#284]\\n   :                 :           :        +- SubqueryAlias `immigration_table`\\n   :                 :           :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, categories_arrival_visa#283, visa_category#284]\\n   :                 :           :              +- Filter (length(arrival_usa_state#282) = 2)\\n   :                 :           :                 +- SubqueryAlias `immigration_table`\\n   :                 :           :                    +- Project [cicid#0, arrival_date#271, i94cit#3 AS immig_country_birth_code#280, i94res#4 AS immig_country_residence_code#281, i94addr#8 AS arrival_usa_state#282, i94visa#11 AS categories_arrival_visa#283, visatype#27 AS visa_category#284]\\n   :                 :           :                       +- SubqueryAlias `immigration_table`\\n   :                 :           :                          +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27, date_add(to_date(1960-01-01, None), cast(arrdate#6 as int)) AS arrival_date#271]\\n   :                 :           :                             +- SubqueryAlias `immigration_table`\\n   :                 :           :                                +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                   +- Filter (i94mode#7 = cast(1 as double))\\n   :                 :           :                                      +- SubqueryAlias `immigration_table`\\n   :                 :           :                                         +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94mode#7, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                            +- SubqueryAlias `immigration_table`\\n   :                 :           :                                               +- Relation[cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] SasRelation(../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0)\\n   :                 :           +- SubqueryAlias `cc_t`\\n   :                 :              +- SubqueryAlias `country_codes_table`\\n   :                 :                 +- LogicalRDD [code#792L, country#793], false\\n   :                 +- SubqueryAlias `cc_t`\\n   :                    +- SubqueryAlias `country_codes_table`\\n   :                       +- LogicalRDD [code#792L, country#793], false\\n   +- SubqueryAlias `cc_t`\\n      +- SubqueryAlias `country_codes_table`\\n         +- LogicalRDD [code#895L, country#896], false\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select imm_t.*, cc_t.country as immig_country_birth\n",
    "from immigration_table imm_t\n",
    "inner join country_codes_table cc_t\n",
    "on imm_t.immig_country_birth_code = cc_t.code\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now adding the residence country from the codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`imm_t.immig_country_residence_code`' given input columns: [imm_t.visa_type, imm_t.immig_country_birth, imm_t.arrival_usa_state, imm_t.cicid, imm_t.immig_country_residence, cc_t.code, cc_t.country, imm_t.visa_category, imm_t.arrival_date]; line 5 pos 3;\\n'Project [ArrayBuffer(imm_t).*, 'cc_t.country AS immig_country_residence#994]\\n+- 'Join Inner, ('imm_t.immig_country_residence_code = code#895L)\\n   :- SubqueryAlias `imm_t`\\n   :  +- SubqueryAlias `immigration_table`\\n   :     +- Project [cicid#0, arrival_date#271, immig_country_birth#803, immig_country_residence#878, arrival_usa_state#282, visa_type#388, visa_category#284]\\n   :        +- SubqueryAlias `immigration_table`\\n   :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, immig_country_birth#803, country#793 AS immig_country_residence#878]\\n   :              +- Join Inner, (immig_country_residence_code#281 = cast(code#792L as double))\\n   :                 :- SubqueryAlias `imm_t`\\n   :                 :  +- SubqueryAlias `immigration_table`\\n   :                 :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, country#793 AS immig_country_birth#803]\\n   :                 :        +- Join Inner, (immig_country_birth_code#280 = cast(code#792L as double))\\n   :                 :           :- SubqueryAlias `imm_t`\\n   :                 :           :  +- SubqueryAlias `immigration_table`\\n   :                 :           :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, CASE WHEN (categories_arrival_visa#283 = cast(1.0 as double)) THEN Business WHEN (categories_arrival_visa#283 = cast(2.0 as double)) THEN Leisure WHEN (categories_arrival_visa#283 = cast(3.0 as double)) THEN Study ELSE Unknown END AS visa_type#388, visa_category#284]\\n   :                 :           :        +- SubqueryAlias `immigration_table`\\n   :                 :           :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, categories_arrival_visa#283, visa_category#284]\\n   :                 :           :              +- Filter (length(arrival_usa_state#282) = 2)\\n   :                 :           :                 +- SubqueryAlias `immigration_table`\\n   :                 :           :                    +- Project [cicid#0, arrival_date#271, i94cit#3 AS immig_country_birth_code#280, i94res#4 AS immig_country_residence_code#281, i94addr#8 AS arrival_usa_state#282, i94visa#11 AS categories_arrival_visa#283, visatype#27 AS visa_category#284]\\n   :                 :           :                       +- SubqueryAlias `immigration_table`\\n   :                 :           :                          +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27, date_add(to_date(1960-01-01, None), cast(arrdate#6 as int)) AS arrival_date#271]\\n   :                 :           :                             +- SubqueryAlias `immigration_table`\\n   :                 :           :                                +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                   +- Filter (i94mode#7 = cast(1 as double))\\n   :                 :           :                                      +- SubqueryAlias `immigration_table`\\n   :                 :           :                                         +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94mode#7, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                            +- SubqueryAlias `immigration_table`\\n   :                 :           :                                               +- Relation[cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] SasRelation(../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0)\\n   :                 :           +- SubqueryAlias `cc_t`\\n   :                 :              +- SubqueryAlias `country_codes_table`\\n   :                 :                 +- LogicalRDD [code#792L, country#793], false\\n   :                 +- SubqueryAlias `cc_t`\\n   :                    +- SubqueryAlias `country_codes_table`\\n   :                       +- LogicalRDD [code#792L, country#793], false\\n   +- SubqueryAlias `cc_t`\\n      +- SubqueryAlias `country_codes_table`\\n         +- LogicalRDD [code#895L, country#896], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`imm_t.immig_country_residence_code`' given input columns: [imm_t.visa_type, imm_t.immig_country_birth, imm_t.arrival_usa_state, imm_t.cicid, imm_t.immig_country_residence, cc_t.code, cc_t.country, imm_t.visa_category, imm_t.arrival_date]; line 5 pos 3;\n'Project [ArrayBuffer(imm_t).*, 'cc_t.country AS immig_country_residence#994]\n+- 'Join Inner, ('imm_t.immig_country_residence_code = code#895L)\n   :- SubqueryAlias `imm_t`\n   :  +- SubqueryAlias `immigration_table`\n   :     +- Project [cicid#0, arrival_date#271, immig_country_birth#803, immig_country_residence#878, arrival_usa_state#282, visa_type#388, visa_category#284]\n   :        +- SubqueryAlias `immigration_table`\n   :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, immig_country_birth#803, country#793 AS immig_country_residence#878]\n   :              +- Join Inner, (immig_country_residence_code#281 = cast(code#792L as double))\n   :                 :- SubqueryAlias `imm_t`\n   :                 :  +- SubqueryAlias `immigration_table`\n   :                 :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, country#793 AS immig_country_birth#803]\n   :                 :        +- Join Inner, (immig_country_birth_code#280 = cast(code#792L as double))\n   :                 :           :- SubqueryAlias `imm_t`\n   :                 :           :  +- SubqueryAlias `immigration_table`\n   :                 :           :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, CASE WHEN (categories_arrival_visa#283 = cast(1.0 as double)) THEN Business WHEN (categories_arrival_visa#283 = cast(2.0 as double)) THEN Leisure WHEN (categories_arrival_visa#283 = cast(3.0 as double)) THEN Study ELSE Unknown END AS visa_type#388, visa_category#284]\n   :                 :           :        +- SubqueryAlias `immigration_table`\n   :                 :           :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, categories_arrival_visa#283, visa_category#284]\n   :                 :           :              +- Filter (length(arrival_usa_state#282) = 2)\n   :                 :           :                 +- SubqueryAlias `immigration_table`\n   :                 :           :                    +- Project [cicid#0, arrival_date#271, i94cit#3 AS immig_country_birth_code#280, i94res#4 AS immig_country_residence_code#281, i94addr#8 AS arrival_usa_state#282, i94visa#11 AS categories_arrival_visa#283, visatype#27 AS visa_category#284]\n   :                 :           :                       +- SubqueryAlias `immigration_table`\n   :                 :           :                          +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27, date_add(to_date(1960-01-01, None), cast(arrdate#6 as int)) AS arrival_date#271]\n   :                 :           :                             +- SubqueryAlias `immigration_table`\n   :                 :           :                                +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27]\n   :                 :           :                                   +- Filter (i94mode#7 = cast(1 as double))\n   :                 :           :                                      +- SubqueryAlias `immigration_table`\n   :                 :           :                                         +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94mode#7, i94addr#8, i94visa#11, visatype#27]\n   :                 :           :                                            +- SubqueryAlias `immigration_table`\n   :                 :           :                                               +- Relation[cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] SasRelation(../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0)\n   :                 :           +- SubqueryAlias `cc_t`\n   :                 :              +- SubqueryAlias `country_codes_table`\n   :                 :                 +- LogicalRDD [code#792L, country#793], false\n   :                 +- SubqueryAlias `cc_t`\n   :                    +- SubqueryAlias `country_codes_table`\n   :                       +- LogicalRDD [code#792L, country#793], false\n   +- SubqueryAlias `cc_t`\n      +- SubqueryAlias `country_codes_table`\n         +- LogicalRDD [code#895L, country#896], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-645c8bbb251d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minner\u001b[0m \u001b[0mjoin\u001b[0m \u001b[0mcountry_codes_table\u001b[0m \u001b[0mcc_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mon\u001b[0m \u001b[0mimm_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimmig_country_residence_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \"\"\").createOrReplaceTempView(\"immigration_table\")\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`imm_t.immig_country_residence_code`' given input columns: [imm_t.visa_type, imm_t.immig_country_birth, imm_t.arrival_usa_state, imm_t.cicid, imm_t.immig_country_residence, cc_t.code, cc_t.country, imm_t.visa_category, imm_t.arrival_date]; line 5 pos 3;\\n'Project [ArrayBuffer(imm_t).*, 'cc_t.country AS immig_country_residence#994]\\n+- 'Join Inner, ('imm_t.immig_country_residence_code = code#895L)\\n   :- SubqueryAlias `imm_t`\\n   :  +- SubqueryAlias `immigration_table`\\n   :     +- Project [cicid#0, arrival_date#271, immig_country_birth#803, immig_country_residence#878, arrival_usa_state#282, visa_type#388, visa_category#284]\\n   :        +- SubqueryAlias `immigration_table`\\n   :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, immig_country_birth#803, country#793 AS immig_country_residence#878]\\n   :              +- Join Inner, (immig_country_residence_code#281 = cast(code#792L as double))\\n   :                 :- SubqueryAlias `imm_t`\\n   :                 :  +- SubqueryAlias `immigration_table`\\n   :                 :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, visa_type#388, visa_category#284, country#793 AS immig_country_birth#803]\\n   :                 :        +- Join Inner, (immig_country_birth_code#280 = cast(code#792L as double))\\n   :                 :           :- SubqueryAlias `imm_t`\\n   :                 :           :  +- SubqueryAlias `immigration_table`\\n   :                 :           :     +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, CASE WHEN (categories_arrival_visa#283 = cast(1.0 as double)) THEN Business WHEN (categories_arrival_visa#283 = cast(2.0 as double)) THEN Leisure WHEN (categories_arrival_visa#283 = cast(3.0 as double)) THEN Study ELSE Unknown END AS visa_type#388, visa_category#284]\\n   :                 :           :        +- SubqueryAlias `immigration_table`\\n   :                 :           :           +- Project [cicid#0, arrival_date#271, immig_country_birth_code#280, immig_country_residence_code#281, arrival_usa_state#282, categories_arrival_visa#283, visa_category#284]\\n   :                 :           :              +- Filter (length(arrival_usa_state#282) = 2)\\n   :                 :           :                 +- SubqueryAlias `immigration_table`\\n   :                 :           :                    +- Project [cicid#0, arrival_date#271, i94cit#3 AS immig_country_birth_code#280, i94res#4 AS immig_country_residence_code#281, i94addr#8 AS arrival_usa_state#282, i94visa#11 AS categories_arrival_visa#283, visatype#27 AS visa_category#284]\\n   :                 :           :                       +- SubqueryAlias `immigration_table`\\n   :                 :           :                          +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27, date_add(to_date(1960-01-01, None), cast(arrdate#6 as int)) AS arrival_date#271]\\n   :                 :           :                             +- SubqueryAlias `immigration_table`\\n   :                 :           :                                +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                   +- Filter (i94mode#7 = cast(1 as double))\\n   :                 :           :                                      +- SubqueryAlias `immigration_table`\\n   :                 :           :                                         +- Project [cicid#0, i94cit#3, i94res#4, arrdate#6, i94mode#7, i94addr#8, i94visa#11, visatype#27]\\n   :                 :           :                                            +- SubqueryAlias `immigration_table`\\n   :                 :           :                                               +- Relation[cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] SasRelation(../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0)\\n   :                 :           +- SubqueryAlias `cc_t`\\n   :                 :              +- SubqueryAlias `country_codes_table`\\n   :                 :                 +- LogicalRDD [code#792L, country#793], false\\n   :                 +- SubqueryAlias `cc_t`\\n   :                    +- SubqueryAlias `country_codes_table`\\n   :                       +- LogicalRDD [code#792L, country#793], false\\n   +- SubqueryAlias `cc_t`\\n      +- SubqueryAlias `country_codes_table`\\n         +- LogicalRDD [code#895L, country#896], false\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select imm_t.*, cc_t.country as immig_country_residence\n",
    "from immigration_table imm_t\n",
    "inner join country_codes_table cc_t\n",
    "on imm_t.immig_country_residence_code = cc_t.code\n",
    "\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Now remove the code columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select cicid, arrival_date, immig_country_birth, immig_country_residence, arrival_usa_state,\n",
    "visa_type, visa_category from immigration_table\"\"\").createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2264187|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from immigration_table limit 5\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Now inserting the data the fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact = spark.sql(\"\"\"select * from immigration_table\"\"\")\n",
    "immigration_fact.createOrReplaceTempView(\"immigration_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Finally for the fact table, write to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact.write.parquet(\"immigration_fact_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Second, the \"country_monthly_temperature_dim\"\n",
    "\n",
    "Using the city_temperature data:\n",
    "- Aggregate the city temperatures to calculate the monthly average temperature for each country.\n",
    "- Load the data to the dimension table.\n",
    "- Write to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------------------+------------------+-----------------------------+-----+-------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+\n",
      "|1963-09-01 00:00:00|            13.164|                         0.29|Århus|Denmark|\n",
      "|1963-10-01 00:00:00|              9.17|                        0.269|Århus|Denmark|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature_cities_spark = spark.createDataFrame(df_temperature_cities[['dt',\n",
    "                                                                           'AverageTemperature',\n",
    "                                                                           'AverageTemperatureUncertainty',\n",
    "                                                                           'City',\n",
    "                                                                           'Country']]) \n",
    "df_temperature_cities_spark.printSchema()\n",
    "df_temperature_cities_spark.show(2)\n",
    "df_temperature_cities_spark.createOrReplaceTempView(\"temperature_city_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### First, aggregate the average temperature to be per country\n",
    "#### Then load the data to the dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_monthly_temperature_dim = \\\n",
    "spark.sql(\"\"\"select month(dt) as month, Country as country, avg(AverageTemperature) as average_temperature\n",
    "from temperature_city_table\n",
    "group by month, country\n",
    "order by country, month\"\"\")\n",
    "\n",
    "country_monthly_temperature_dim.createOrReplaceTempView(\"country_monthly_temperature_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------------+\n",
      "|month|    country|average_temperature|\n",
      "+-----+-----------+-------------------+\n",
      "|    1|Afghanistan| 1.2564550000000008|\n",
      "|    2|Afghanistan| 3.3597075000000007|\n",
      "|    3|Afghanistan|  8.879800000000008|\n",
      "|    4|Afghanistan| 14.820217500000004|\n",
      "|    5|Afghanistan| 20.322742499999993|\n",
      "|    6|Afghanistan| 25.062480000000015|\n",
      "|    7|Afghanistan| 26.469322499999993|\n",
      "|    8|Afghanistan| 25.051309999999987|\n",
      "|    9|Afghanistan| 20.835020000000007|\n",
      "|   10|Afghanistan| 14.701605000000008|\n",
      "|   11|Afghanistan|            8.77815|\n",
      "|   12|Afghanistan|  3.517519999999999|\n",
      "|    1|    Albania|  8.439160000000001|\n",
      "|    2|    Albania|  8.937439999999997|\n",
      "|    3|    Albania| 10.927040000000003|\n",
      "|    4|    Albania| 13.595800000000004|\n",
      "|    5|    Albania| 18.187219999999996|\n",
      "|    6|    Albania|            21.9605|\n",
      "|    7|    Albania|  24.31614000000002|\n",
      "|    8|    Albania| 24.315879999999996|\n",
      "+-----+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from country_monthly_temperature_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Finally write this table to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_monthly_temperature_dim.write.parquet(\"country_monthly_temperature_dim_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Third, for \"state_monthly_temperature_dim\"\n",
    "- Map the states to state_codes using city_demographics to be inserted in the dimension table as alpha codes for each state.\n",
    "- Load the data to the dimension table.\n",
    "- Write to parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Convert demographics to a spark dataframe to be able to join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n",
      "+-------------+----------+-------------+\n",
      "|         City|State_Code|        State|\n",
      "+-------------+----------+-------------+\n",
      "|Silver Spring|        MD|     Maryland|\n",
      "|       Quincy|        MA|Massachusetts|\n",
      "+-------------+----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|  State|      Country|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+\n",
      "|1963-09-01 00:00:00|            23.083|          0.28300000000000003|Alabama|United States|\n",
      "|1963-10-01 00:00:00|            18.871|                        0.239|Alabama|United States|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_demo_spark = spark.createDataFrame(df_city_demo) \n",
    "df_city_demo_spark.printSchema()\n",
    "df_city_demo_spark.show(2)\n",
    "df_city_demo_spark.createOrReplaceTempView(\"city_demo_spark\")\n",
    "\n",
    "df_temperature_usstates_spark = spark.createDataFrame(df_temperature_usstates)\n",
    "df_temperature_usstates_spark.printSchema()\n",
    "df_temperature_usstates_spark.show(2)\n",
    "df_temperature_usstates_spark.createOrReplaceTempView(\"temperature_state_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Calculate the average State temperature per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_state_table = \\\n",
    "spark.sql(\"\"\"select month(dt) as month, State as state, avg(AverageTemperature) as average_temperature\n",
    "from temperature_state_table\n",
    "group by month, state\n",
    "order by state, month\"\"\")\n",
    "\n",
    "temperature_state_table.createOrReplaceTempView(\"temperature_state_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------------------+\n",
      "|month|  state|average_temperature|\n",
      "+-----+-------+-------------------+\n",
      "|    1|Alabama|            7.02144|\n",
      "|    2|Alabama|  8.817859999999998|\n",
      "|    3|Alabama| 13.237700000000002|\n",
      "|    4|Alabama|           17.42764|\n",
      "|    5|Alabama| 21.698400000000007|\n",
      "|    6|Alabama| 25.478219999999997|\n",
      "|    7|Alabama|           26.99136|\n",
      "|    8|Alabama| 26.617319999999996|\n",
      "|    9|Alabama| 23.761980392156865|\n",
      "|   10|Alabama| 17.646179999999998|\n",
      "|   11|Alabama| 12.616119999999999|\n",
      "|   12|Alabama|  8.330459999999995|\n",
      "|    1| Alaska|-19.071919999999995|\n",
      "|    2| Alaska|          -16.97936|\n",
      "|    3| Alaska|-13.393739999999998|\n",
      "|    4| Alaska|           -5.79734|\n",
      "|    5| Alaska|            3.31124|\n",
      "|    6| Alaska|            9.87654|\n",
      "|    7| Alaska| 12.050160000000004|\n",
      "|    8| Alaska|            9.97292|\n",
      "+-----+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from temperature_state_table\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_state_monthly_temperature_dim = spark.sql(\"\"\"\n",
    "select ts_t.month, cds.State_Code as state_alpha_code, ts_t.average_temperature as average_temperature\n",
    "from temperature_state_table ts_t\n",
    "inner join city_demo_spark cds\n",
    "on ts_t.state = cds.State\n",
    "group by month, state_alpha_code, average_temperature\n",
    "order by state_alpha_code, month\n",
    "\"\"\")\n",
    "\n",
    "us_state_monthly_temperature_dim.createOrReplaceTempView(\"us_state_monthly_temperature_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-------------------+\n",
      "|month|state_alpha_code|average_temperature|\n",
      "+-----+----------------+-------------------+\n",
      "|    1|              AK|-19.071919999999995|\n",
      "|    2|              AK|          -16.97936|\n",
      "|    3|              AK|-13.393739999999998|\n",
      "|    4|              AK|           -5.79734|\n",
      "|    5|              AK|            3.31124|\n",
      "|    6|              AK|            9.87654|\n",
      "|    7|              AK| 12.050160000000004|\n",
      "|    8|              AK|            9.97292|\n",
      "|    9|              AK|  4.590639999999999|\n",
      "|   10|              AK|            -4.8628|\n",
      "|   11|              AK|-13.426379999999995|\n",
      "|   12|              AK|          -17.49882|\n",
      "|    1|              AL|            7.02144|\n",
      "|    2|              AL|  8.817859999999998|\n",
      "|    3|              AL| 13.237700000000002|\n",
      "|    4|              AL|           17.42764|\n",
      "|    5|              AL| 21.698400000000007|\n",
      "|    6|              AL| 25.478219999999997|\n",
      "|    7|              AL|           26.99136|\n",
      "|    8|              AL| 26.617319999999996|\n",
      "+-----+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from us_state_monthly_temperature_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Finally, write the state_monthly_temperature to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_state_monthly_temperature_dim.write.parquet(\"us_state_monthly_temperature_dim_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Fourth \"usa_states_dim\", the next steps are\n",
    "Using the demographics data (All Not Yet Done):\n",
    "- Filter and get the wanted columns (the state code and the state name).\n",
    "- Load the data to the dimension table.\n",
    "- Write to parquet.\n",
    "\n",
    "### But since we already have a spark df and a table, we can directly filter from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "usa_states_dim = \\\n",
    "spark.sql(\"\"\"select distinct(State_Code) as state_alpha_code, State as state\n",
    "from city_demo_spark\"\"\")\n",
    "\n",
    "usa_states_dim.createOrReplaceTempView(\"usa_states_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|state_alpha_code|               state|\n",
      "+----------------+--------------------+\n",
      "|              MT|             Montana|\n",
      "|              NC|      North Carolina|\n",
      "|              MD|            Maryland|\n",
      "|              CO|            Colorado|\n",
      "|              CT|         Connecticut|\n",
      "|              IL|            Illinois|\n",
      "|              NJ|          New Jersey|\n",
      "|              DE|            Delaware|\n",
      "|              DC|District of Columbia|\n",
      "|              AR|            Arkansas|\n",
      "|              TN|           Tennessee|\n",
      "|              LA|           Louisiana|\n",
      "|              AK|              Alaska|\n",
      "|              CA|          California|\n",
      "|              NM|          New Mexico|\n",
      "|              UT|                Utah|\n",
      "|              MI|            Michigan|\n",
      "|              NY|            New York|\n",
      "|              NH|       New Hampshire|\n",
      "|              WA|          Washington|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from usa_states_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "usa_states_dim.write.parquet(\"usa_states_dim_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now, for the last dimension table \"arrival_dates_dim\"\n",
    "Using the immigration data\n",
    "- Extract all the columns from the arrival date (year, month, week, day, day_of_week and season)\n",
    "- Load the data to the dimension table.\n",
    "- Write to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "arrival_dates_dim = spark.sql(\"\"\"\n",
    "select distinct(arrival_date) as date, year(arrival_date) as year, month(arrival_date) as month,\n",
    "weekofyear(arrival_date) as week, day(arrival_date) as day, date_format(arrival_date, 'EEEE') as day_of_week,\n",
    "case when month(arrival_date) in (12, 1, 2) then 'Winter'\n",
    "     when month(arrival_date) in (3, 4, 5) then 'Spring'\n",
    "     when month(arrival_date) in (6, 7, 8) then 'Summer'\n",
    "     when month(arrival_date) in (9, 10, 11) then 'Autumn'\n",
    "     else 'date_error'\n",
    "     end as season\n",
    "from immigration_fact\"\"\")\n",
    "\n",
    "arrival_dates_dim.createOrReplaceTempView(\"arrival_dates_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "arrival_dates_dim.write.parquet(\"arrival_dates_dim_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### First, we'll check key uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|(count(1) - count(DISTINCT cicid))|\n",
      "+----------------------------------+\n",
      "|                                 0|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) - count(distinct(cicid)) from immigration_fact\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|(count(1) - count(DISTINCT named_struct(month, month, country, country)))|\n",
      "+-------------------------------------------------------------------------+\n",
      "|                                                                        0|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) - count(distinct(month, country)) from country_monthly_temperature_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------+\n",
      "|(count(1) - count(DISTINCT named_struct(month, month, state_alpha_code, state_alpha_code)))|\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "|                                                                                          0|\n",
      "+-------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) - count(distinct(month, state_alpha_code)) from us_state_monthly_temperature_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|(count(1) - count(DISTINCT state_alpha_code))|\n",
      "+---------------------------------------------+\n",
      "|                                            0|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) - count(distinct(state_alpha_code)) from usa_states_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|(count(1) - count(DISTINCT date))|\n",
      "+---------------------------------+\n",
      "|                                0|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*) - count(distinct(date)) from arrival_dates_dim\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Then we will do sort of functional tests by answering some of the questions related to the scope of the project, as the following:\n",
    "\n",
    "1- What is the most nationality/residents that visits the US? <br />\n",
    "2- Temperatures of the countries of the 5 most visiting nationalities, and temperature of destination us states? <br />\n",
    "3- Most popular visa types? <br />\n",
    "4- Which US state are most visited? <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### To answer the first question \n",
    "1- What is the most nationality/residents that visits the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+--------------------------+\n",
      "|immig_country_birth|immig_country_residence|count(immig_country_birth)|\n",
      "+-------------------+-----------------------+--------------------------+\n",
      "|         CHINA, PRC|             CHINA, PRC|                    261708|\n",
      "|              JAPAN|                  JAPAN|                    207688|\n",
      "|     UNITED KINGDOM|         UNITED KINGDOM|                    198300|\n",
      "|             BRAZIL|                 BRAZIL|                    180344|\n",
      "|            MEXICO |                MEXICO |                    149357|\n",
      "|          AUSTRALIA|              AUSTRALIA|                     91034|\n",
      "|              INDIA|                  INDIA|                     88506|\n",
      "|             FRANCE|                 FRANCE|                     70010|\n",
      "|         ARGENTINA |             ARGENTINA |                     65343|\n",
      "|              ITALY|                  ITALY|                     54415|\n",
      "|           COLOMBIA|               COLOMBIA|                     49209|\n",
      "|             TAIWAN|                 TAIWAN|                     36810|\n",
      "|              SPAIN|                  SPAIN|                     35700|\n",
      "|       SAUDI ARABIA|           SAUDI ARABIA|                     35692|\n",
      "|             SWEDEN|                 SWEDEN|                     34039|\n",
      "|          VENEZUELA|              VENEZUELA|                     32114|\n",
      "|        NETHERLANDS|            NETHERLANDS|                     31540|\n",
      "|         COSTA RICA|             COSTA RICA|                     27859|\n",
      "|              CHILE|                  CHILE|                     23389|\n",
      "|             ISRAEL|                 ISRAEL|                     20895|\n",
      "+-------------------+-----------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select immig_country_birth, immig_country_residence, count(immig_country_birth)\n",
    "from immigration_fact \n",
    "group by immig_country_birth, immig_country_residence\n",
    "order by count(immig_country_birth) desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### It would be interesting to divide the values by the population of each country because I expected China of course, nevertheless, UK, Japan and France are interesting to see!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Second question\n",
    "2- Temperatures of the countries of the 5 most visiting nationalities, and temperature of destination us states?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Since the engine kept failing when I use order by to the sql, I decided to make a workaround"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "spark.sql(\"\"\"select distinct(immig_country_residence), count(immig_country_residence) as count_countries,\n",
    "cmt_d.average_temperature as average_temperature_departure_country, arrival_usa_state,\n",
    "us_smt.average_temperature as arrival_state_average_temperature\n",
    "from immigration_fact \n",
    "inner join country_monthly_temperature_dim cmt_d on\n",
    "lower(immigration_fact.immig_country_residence) = lower(cmt_d.country)\n",
    "and month(immigration_fact.arrival_date) = cmt_d.month\n",
    "inner join us_state_monthly_temperature_dim us_smt on\n",
    "immigration_fact.arrival_usa_state = us_smt.state_alpha_code\n",
    "and month(immigration_fact.arrival_date) = us_smt.month\n",
    "group by immig_country_residence, cmt_d.average_temperature, arrival_usa_state, arrival_state_average_temperature\n",
    "order by count_countries desc limit 10 \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### It looks like people from Japan go to Hawaei to enjoy the Sun! :D, Brazil to Florida to enjoy some cold. California is popular too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Third question\n",
    "3- Most popular visa types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|visa_type|visa_category|count(1)|\n",
      "+---------+-------------+--------+\n",
      "|  Leisure|           B2|  834384|\n",
      "|  Leisure|           WT|  655625|\n",
      "|    Study|           F1|  327439|\n",
      "| Business|           WB|  226348|\n",
      "| Business|           B1|  175953|\n",
      "| Business|           E2|   22395|\n",
      "|    Study|           F2|    8348|\n",
      "|  Leisure|          GMT|    3967|\n",
      "| Business|           E1|    3893|\n",
      "| Business|            I|    2842|\n",
      "+---------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select visa_type, visa_category, count(*) from\n",
    "immigration_fact\n",
    "group by visa_type, visa_category\n",
    "order by count(*) desc limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### So Leisure in general is the most popular then Study, makes sense. It would be super interesting to see the relation of the visas with the destination usa_state, and even the departure countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+-----------------+\n",
      "|visa_type|visa_category|count(1)|arrival_usa_state|\n",
      "+---------+-------------+--------+-----------------+\n",
      "|  Leisure|           B2|  357198|               FL|\n",
      "|  Leisure|           WT|  149478|               FL|\n",
      "|  Leisure|           WT|  140001|               HI|\n",
      "|  Leisure|           B2|  127089|               CA|\n",
      "|  Leisure|           WT|  114902|               NY|\n",
      "|  Leisure|           WT|  102351|               CA|\n",
      "|  Leisure|           B2|   93811|               NY|\n",
      "|    Study|           F1|   57644|               CA|\n",
      "| Business|           WB|   51244|               CA|\n",
      "|  Leisure|           B2|   44874|               TX|\n",
      "+---------+-------------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select visa_type, visa_category, count(*), arrival_usa_state from\n",
    "immigration_fact\n",
    "group by arrival_usa_state, visa_type, visa_category\n",
    "order by count(*) desc limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+-----------------------+\n",
      "|visa_type|visa_category|count(1)|immig_country_residence|\n",
      "+---------+-------------+--------+-----------------------+\n",
      "|  Leisure|           B2|  161227|                 BRAZIL|\n",
      "|  Leisure|           WT|  159931|                  JAPAN|\n",
      "|  Leisure|           WT|  127095|         UNITED KINGDOM|\n",
      "|  Leisure|           B2|  116773|                MEXICO |\n",
      "|  Leisure|           B2|  115571|             CHINA, PRC|\n",
      "|    Study|           F1|  111072|             CHINA, PRC|\n",
      "|  Leisure|           WT|   77822|              AUSTRALIA|\n",
      "| Business|           WB|   62819|         UNITED KINGDOM|\n",
      "|  Leisure|           B2|   61190|             ARGENTINA |\n",
      "|    Study|           F1|   44561|                  INDIA|\n",
      "+---------+-------------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select visa_type, visa_category, count(*), immig_country_residence from\n",
    "immigration_fact\n",
    "group by immig_country_residence, visa_type, visa_category\n",
    "order by count(*) desc limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### The fourth and final question\n",
    "4- Which US state are most visited?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------+\n",
      "|state_alpha_code|         state|count(1)|\n",
      "+----------------+--------------+--------+\n",
      "|              FL|       Florida|  576767|\n",
      "|              CA|    California|  382082|\n",
      "|              NY|      New York|  301714|\n",
      "|              HI|        Hawaii|  159077|\n",
      "|              TX|         Texas|  111657|\n",
      "|              NV|        Nevada|   94348|\n",
      "|              MA| Massachusetts|   57573|\n",
      "|              IL|      Illinois|   46218|\n",
      "|              NJ|    New Jersey|   37774|\n",
      "|              GA|       Georgia|   36127|\n",
      "|              CO|      Colorado|   33795|\n",
      "|              WA|    Washington|   32391|\n",
      "|              PA|  Pennsylvania|   28907|\n",
      "|              MI|      Michigan|   26600|\n",
      "|              AZ|       Arizona|   22204|\n",
      "|              VA|      Virginia|   22007|\n",
      "|              OH|          Ohio|   20667|\n",
      "|              NE|      Nebraska|   17588|\n",
      "|              NC|North Carolina|   17439|\n",
      "|              MD|      Maryland|   15756|\n",
      "+----------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select us_d.state_alpha_code, us_d.state, count(*) \n",
    "from immigration_fact\n",
    "inner join usa_states_dim us_d on\n",
    "arrival_usa_state = us_d.state_alpha_code\n",
    "group by us_d.state_alpha_code, us_d.state\n",
    "order by count(*) desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Go Florida, Go California! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data dictonary is detailed in section 3.1, nevertheless, I will include a file data_dictionary.txt explaining it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### User persona: Such data model and analytics can be used by peopluale that are interested in knowing what features makes travellers actually travel, such as Airline companies, or transportation companies in general!. Knowing what countries posses an immigrating power or even generating high number of tourists is something that can be used by the foreign affair that can build a special relation with such countries and maybe even push for tourist discounts/ads or scholarship for students. Also, cities/states that attract a specific type of visa can give many insights to the diversity nature of the city, its attraction to investments and its touristic nature and in what season!. I think that users that can benefit the most of this are travel agencies, but there are also a broad variety of institutions that can benefit from these types of analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Choice of Tools: Through working with the project, working with all the dataset was seemless easy (fast) except for the immigration dataset, because the data is huge for each month!. The only logical choise would be Spark as the data will not fit on RAM. I used Pandas for quick work with small datasets while used spark with big ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data storage and update: This data should be stored on something like s3, and it should be updated in batches as there is no need to stream the data (not time sensitive - unless there is a crisis on hand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### - Multiusage and increase in data: The same setup purposal can be applied, where data is saved and updated on S3, we would use spark as a processing platform after we construct the staging tables as we did in previous projects. \n",
    "\n",
    "##### -  For automating the process Aiflow is a good choice and can trigger the 7-AM actions by also breaking up the actions and this notebook into smaller chunks/tasks. We could also use simpler approaches like AWS cloudwatch crone job triggers that can trigger smaller functions based on AWS lambda.\n",
    "\n",
    "##### - Finally, if the database needs to be accessed by alot of people then our database can be hosted on a redshift cluster and there are options for scalability flexibility so it would be perfect in case we can expect an increase in usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
